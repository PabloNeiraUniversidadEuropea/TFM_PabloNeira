{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq = pd.read_csv(\"nasdaqdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_excel(\"TweetsScrapped.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating flags and initial treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq['Date'] = pd.to_datetime(nasdaq['Date'], format='%b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq = nasdaq.sort_values(by='Date', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Crear una columna 'Tendencia' con el valor por defecto 'Sin Cambio'\n",
    "nasdaq['Tendence'] = 'Sin Cambio'\n",
    "nasdaq['Tendence'][0] = 1\n",
    "# Calcular la tendencia en base a la cotización del día anterior\n",
    "for i in range(1, len(nasdaq)):\n",
    "    if nasdaq['Close'].iloc[i] > nasdaq['Close'].iloc[i - 1]:\n",
    "        nasdaq['Tendence'].iloc[i] = 1\n",
    "    elif nasdaq['Close'].iloc[i] < nasdaq['Close'].iloc[i - 1]:\n",
    "        nasdaq['Tendence'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Tendence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>15,832.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>15,622.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>15,100.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>15,080.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>14,935.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>10,497.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>10,353.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>10,213.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>10,478.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>10,466.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Close  Tendence\n",
       "0   2022-01-03  15,832.80         1\n",
       "1   2022-01-04  15,622.72         0\n",
       "2   2022-01-05  15,100.17         0\n",
       "3   2022-01-06  15,080.86         0\n",
       "4   2022-01-07  14,935.90         0\n",
       "..         ...        ...       ...\n",
       "246 2022-12-23  10,497.86         1\n",
       "247 2022-12-27  10,353.23         0\n",
       "248 2022-12-28  10,213.29         0\n",
       "249 2022-12-29  10,478.09         1\n",
       "250 2022-12-30  10,466.48         0\n",
       "\n",
       "[251 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Fecha_2'] = pd.to_datetime(tweets['Fecha_2'], format='%b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_new = tweets[['Tweets', 'Fecha_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Fecha_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FIRE  #shinja $shinja</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2021 returns:\\n\\n• Ethereum: 403% \\n• Bitcoin:...</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Let's make the biggest comeback ever in 2022 #...</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Best coin</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@NSAVTech &amp; @Vagabondappio : Another vertical ...</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19826</td>\n",
       "      <td>The Nasdaq had its worst year since 2008. Thes...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19827</td>\n",
       "      <td>The year in stock indexes:\\n\\n—Dow Jones 30:  ...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19828</td>\n",
       "      <td>The @SECGov recently approved @NasdaqExchange’...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19829</td>\n",
       "      <td>Client protection begins with account securit...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19830</td>\n",
       "      <td>There were few places for investors to hide in...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweets    Fecha_2\n",
       "0                                  FIRE  #shinja $shinja 2022-01-01\n",
       "1      2021 returns:\\n\\n• Ethereum: 403% \\n• Bitcoin:... 2022-01-01\n",
       "2      Let's make the biggest comeback ever in 2022 #... 2022-01-01\n",
       "3                                              Best coin 2022-01-01\n",
       "4      @NSAVTech & @Vagabondappio : Another vertical ... 2022-01-01\n",
       "...                                                  ...        ...\n",
       "19826  The Nasdaq had its worst year since 2008. Thes... 2022-12-31\n",
       "19827  The year in stock indexes:\\n\\n—Dow Jones 30:  ... 2022-12-31\n",
       "19828  The @SECGov recently approved @NasdaqExchange’... 2022-12-31\n",
       "19829   Client protection begins with account securit... 2022-12-31\n",
       "19830  There were few places for investors to hide in... 2022-12-31\n",
       "\n",
       "[19831 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(tweets_new, nasdaq, left_on='Fecha_2', right_on='Date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[['Date', 'Tweets', 'Tendence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pabloneira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Limpieza y preprocesamiento de texto\n",
    "df_final['Tweets'] = df_final['Tweets'].str.lower()  # Convertir a minúsculas\n",
    "df_final['Tweets'] = df_final['Tweets'].str.replace('[^a-zA-Z\\s]', '', regex=True)  # Eliminar caracteres especiales y números\n",
    "df_final['Tweets'] = df_final['Tweets'].str.split()  # Tokenización\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_final['Tweets'] = df_final['Tweets'].apply(lambda x: [word for word in x if word not in stop_words])  # Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Agregación de tweets por día\n",
    "df_final = df_final.groupby('Date').agg({'Tweets': 'sum', 'Tendence': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tendence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>[risk, dow, spx, nasdaq, russell, fang, igopen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>[friction, gone, app, difference, rob, wasnt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>[key, potential, support, levels, watch, year,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>[thanks, nasdaq, looking, forward, day, announ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[fed, balance, sheet, vs, nasdaq, amd, fb, ide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>[horrible, lot, accomplished, chipotle, huge, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>[non, stop, volt, voltichange, imagine, catchi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>[nasdaq, sound, uk, nwbo, back, big, board, ny...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>[deflationary, risk, market, x, sales, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>[skill, everything, deriv, account, synthetic,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                                             Tweets  Tendence\n",
       "0   2022-01-03  [risk, dow, spx, nasdaq, russell, fang, igopen...         1\n",
       "1   2022-01-04  [friction, gone, app, difference, rob, wasnt, ...         0\n",
       "2   2022-01-05  [key, potential, support, levels, watch, year,...         0\n",
       "3   2022-01-06  [thanks, nasdaq, looking, forward, day, announ...         0\n",
       "4   2022-01-07  [fed, balance, sheet, vs, nasdaq, amd, fb, ide...         0\n",
       "..         ...                                                ...       ...\n",
       "246 2022-12-23  [horrible, lot, accomplished, chipotle, huge, ...         1\n",
       "247 2022-12-27  [non, stop, volt, voltichange, imagine, catchi...         0\n",
       "248 2022-12-28  [nasdaq, sound, uk, nwbo, back, big, board, ny...         0\n",
       "249 2022-12-29  [deflationary, risk, market, x, sales, start, ...         1\n",
       "250 2022-12-30  [skill, everything, deriv, account, synthetic,...         0\n",
       "\n",
       "[251 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Análisis de sentimiento agregado\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(' '.join(text))\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "df_final['Sentiment'] = df_final['Tweets'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasdaq: 14497\n",
      "sp: 2688\n",
      "stocks: 1842\n",
      "market: 1745\n",
      "dow: 1706\n",
      "today: 1423\n",
      "us: 1315\n",
      "day: 1214\n",
      "points: 1016\n",
      "since: 963\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado 'df' con una columna 'tokenized_tweets'.\n",
    "# Puedes combinar todas las listas de palabras en una sola lista.\n",
    "\n",
    "all_words = []\n",
    "for word_list in df_final['Tweets']:\n",
    "    all_words.extend(word_list)\n",
    "\n",
    "# Luego, cuentas la frecuencia de cada palabra.\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Puedes obtener las 10 palabras más comunes.\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# 'top_10_words' contendrá una lista de tuplas, donde cada tupla es (palabra, frecuencia).\n",
    "\n",
    "# Si deseas imprimir las palabras más comunes:\n",
    "for word, frequency in top_10_words:\n",
    "    print(f'{word}: {frequency}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Separar los datos en conjuntos de entrenamiento y prueba\n",
    "X = df_final['Sentiment'].values\n",
    "y = df_final['Tendence'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Creación de la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/fixes.py:230: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if _joblib.__version__ >= LooseVersion('0.12'):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/fixes.py:230: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if _joblib.__version__ >= LooseVersion('0.12'):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Random Forest**\n",
      "Accuracy: 0.49\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bajista       0.44      0.52      0.48        23\n",
      "     Alcista       0.54      0.46      0.50        28\n",
      "\n",
      "    accuracy                           0.49        51\n",
      "   macro avg       0.49      0.49      0.49        51\n",
      "weighted avg       0.50      0.49      0.49        51\n",
      "\n",
      "\n",
      "\n",
      "**Support Vector Machine**\n",
      "Accuracy: 0.45\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bajista       0.45      1.00      0.62        23\n",
      "     Alcista       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.45        51\n",
      "   macro avg       0.23      0.50      0.31        51\n",
      "weighted avg       0.20      0.45      0.28        51\n",
      "\n",
      "\n",
      "\n",
      "**Logistic Regression**\n",
      "Accuracy: 0.45\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bajista       0.45      1.00      0.62        23\n",
      "     Alcista       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.45        51\n",
      "   macro avg       0.23      0.50      0.31        51\n",
      "weighted avg       0.20      0.45      0.28        51\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = df_final[['Sentiment']]\n",
    "y = df_final['Tendence']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar los clasificadores\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada clasificador\n",
    "results = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Bajista', 'Alcista'])\n",
    "    results[name] = {'Accuracy': accuracy, 'Classification Report': report}\n",
    "\n",
    "# Mostrar resultados\n",
    "for name, result in results.items():\n",
    "    print(f'**{name}**')\n",
    "    print(f'Accuracy: {result[\"Accuracy\"]:.2f}')\n",
    "    print('Classification Report:')\n",
    "    print(result['Classification Report'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "2/2 [==============================] - 1s 4ms/step - loss: 0.6929 - accuracy: 0.4550\n",
      "Epoch 2/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.6300\n",
      "Epoch 3/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5850\n",
      "Epoch 4/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5850\n",
      "Epoch 5/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5850\n",
      "Epoch 6/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5850\n",
      "Epoch 7/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6837 - accuracy: 0.5850\n",
      "Epoch 8/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5850\n",
      "Epoch 9/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5850\n",
      "Epoch 10/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6804 - accuracy: 0.5850\n",
      "Epoch 11/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6800 - accuracy: 0.5850\n",
      "Epoch 12/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6790 - accuracy: 0.5850\n",
      "Epoch 13/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6784 - accuracy: 0.5850\n",
      "Epoch 14/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6777 - accuracy: 0.5850\n",
      "Epoch 15/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6771 - accuracy: 0.5850\n",
      "Epoch 16/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6766 - accuracy: 0.5850\n",
      "Epoch 17/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.5850\n",
      "Epoch 18/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6757 - accuracy: 0.5850\n",
      "Epoch 19/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.5850\n",
      "Epoch 20/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6750 - accuracy: 0.5850\n",
      "Epoch 21/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.5850\n",
      "Epoch 22/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6743 - accuracy: 0.5850\n",
      "Epoch 23/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.5850\n",
      "Epoch 24/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5850\n",
      "Epoch 25/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6735 - accuracy: 0.5850\n",
      "Epoch 26/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6733 - accuracy: 0.5850\n",
      "Epoch 27/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.5850\n",
      "Epoch 28/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.5850\n",
      "Epoch 29/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6723 - accuracy: 0.5850\n",
      "Epoch 30/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5850\n",
      "Epoch 31/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6716 - accuracy: 0.5850\n",
      "Epoch 32/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.5850\n",
      "Epoch 33/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6711 - accuracy: 0.5850\n",
      "Epoch 34/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.5850\n",
      "Epoch 35/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6705 - accuracy: 0.5850\n",
      "Epoch 36/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.5850\n",
      "Epoch 37/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6699 - accuracy: 0.5850\n",
      "Epoch 38/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5850\n",
      "Epoch 39/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6694 - accuracy: 0.5850\n",
      "Epoch 40/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.5850\n",
      "Epoch 41/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.5850\n",
      "Epoch 42/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5850\n",
      "Epoch 43/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6682 - accuracy: 0.5850\n",
      "Epoch 44/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5850\n",
      "Epoch 45/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5850\n",
      "Epoch 46/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.5850\n",
      "Epoch 47/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5850\n",
      "Epoch 48/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6669 - accuracy: 0.5850\n",
      "Epoch 49/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.5850\n",
      "Epoch 50/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.5850\n",
      "Epoch 51/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.5850\n",
      "Epoch 52/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5850\n",
      "Epoch 53/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6655 - accuracy: 0.5850\n",
      "Epoch 54/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.5850\n",
      "Epoch 55/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6650 - accuracy: 0.5850\n",
      "Epoch 56/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6646 - accuracy: 0.5850\n",
      "Epoch 57/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5850\n",
      "Epoch 58/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6641 - accuracy: 0.5850\n",
      "Epoch 59/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.5850\n",
      "Epoch 60/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6636 - accuracy: 0.5850\n",
      "Epoch 61/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6634 - accuracy: 0.5850\n",
      "Epoch 62/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.5850\n",
      "Epoch 63/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.5900\n",
      "Epoch 64/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6626 - accuracy: 0.5900\n",
      "Epoch 65/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.5900\n",
      "Epoch 66/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.5900\n",
      "Epoch 67/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.5900\n",
      "Epoch 68/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.5900\n",
      "Epoch 69/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6614 - accuracy: 0.5900\n",
      "Epoch 70/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.5900\n",
      "Epoch 71/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6607 - accuracy: 0.5950\n",
      "Epoch 72/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.5950\n",
      "Epoch 73/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.5950\n",
      "Epoch 74/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.5950\n",
      "Epoch 75/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6597 - accuracy: 0.5950\n",
      "Epoch 76/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.5950\n",
      "Epoch 77/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6592 - accuracy: 0.5950\n",
      "Epoch 78/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6590 - accuracy: 0.5950\n",
      "Epoch 79/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.5900\n",
      "Epoch 80/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.5900\n",
      "Epoch 81/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.5900\n",
      "Epoch 82/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6580 - accuracy: 0.5950\n",
      "Epoch 83/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6577 - accuracy: 0.5950\n",
      "Epoch 84/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6576 - accuracy: 0.5950\n",
      "Epoch 85/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6573 - accuracy: 0.5950\n",
      "Epoch 86/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.5950\n",
      "Epoch 87/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6568 - accuracy: 0.5950\n",
      "Epoch 88/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6566 - accuracy: 0.5950\n",
      "Epoch 89/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6564 - accuracy: 0.5950\n",
      "Epoch 90/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.5950\n",
      "Epoch 91/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.5950\n",
      "Epoch 92/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5950\n",
      "Epoch 93/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6554 - accuracy: 0.5950\n",
      "Epoch 94/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.5950\n",
      "Epoch 95/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.5950\n",
      "Epoch 96/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6547 - accuracy: 0.5950\n",
      "Epoch 97/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.5950\n",
      "Epoch 98/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.5950\n",
      "Epoch 99/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.5950\n",
      "Epoch 100/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.5950\n",
      "Epoch 101/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.5950\n",
      "Epoch 102/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6534 - accuracy: 0.5950\n",
      "Epoch 103/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6531 - accuracy: 0.6000\n",
      "Epoch 104/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6529 - accuracy: 0.5950\n",
      "Epoch 105/700\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6527 - accuracy: 0.5950\n",
      "Epoch 106/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6524 - accuracy: 0.5950\n",
      "Epoch 107/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.5950\n",
      "Epoch 108/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.6000\n",
      "Epoch 109/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6518 - accuracy: 0.6000\n",
      "Epoch 110/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6516 - accuracy: 0.6000\n",
      "Epoch 111/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.6150\n",
      "Epoch 112/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6512 - accuracy: 0.6150\n",
      "Epoch 113/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6510 - accuracy: 0.6150\n",
      "Epoch 114/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6507 - accuracy: 0.6200\n",
      "Epoch 115/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.6150\n",
      "Epoch 116/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6504 - accuracy: 0.6100\n",
      "Epoch 117/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6502 - accuracy: 0.6100\n",
      "Epoch 118/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6100\n",
      "Epoch 119/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6498 - accuracy: 0.6150\n",
      "Epoch 120/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6496 - accuracy: 0.6150\n",
      "Epoch 121/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6494 - accuracy: 0.6150\n",
      "Epoch 122/700\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.61 - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6150\n",
      "Epoch 123/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6490 - accuracy: 0.6150\n",
      "Epoch 124/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6490 - accuracy: 0.6150\n",
      "Epoch 125/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6487 - accuracy: 0.6200\n",
      "Epoch 126/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6485 - accuracy: 0.6200\n",
      "Epoch 127/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6483 - accuracy: 0.6250\n",
      "Epoch 128/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6482 - accuracy: 0.6200\n",
      "Epoch 129/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.6250\n",
      "Epoch 130/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6477 - accuracy: 0.6150\n",
      "Epoch 131/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6476 - accuracy: 0.6150\n",
      "Epoch 132/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6473 - accuracy: 0.6150\n",
      "Epoch 133/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6472 - accuracy: 0.6200\n",
      "Epoch 134/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6472 - accuracy: 0.6150\n",
      "Epoch 135/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6469 - accuracy: 0.6150\n",
      "Epoch 136/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6468 - accuracy: 0.6150\n",
      "Epoch 137/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6467 - accuracy: 0.6100\n",
      "Epoch 138/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.6100\n",
      "Epoch 139/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6462 - accuracy: 0.6200\n",
      "Epoch 140/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.6100\n",
      "Epoch 141/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6459 - accuracy: 0.6100\n",
      "Epoch 142/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6459 - accuracy: 0.6100\n",
      "Epoch 143/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6456 - accuracy: 0.6100\n",
      "Epoch 144/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6455 - accuracy: 0.6150\n",
      "Epoch 145/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6454 - accuracy: 0.6150\n",
      "Epoch 146/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.6100\n",
      "Epoch 147/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6100\n",
      "Epoch 148/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.6100\n",
      "Epoch 149/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.6100\n",
      "Epoch 150/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6100\n",
      "Epoch 151/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6445 - accuracy: 0.6100\n",
      "Epoch 152/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6100\n",
      "Epoch 153/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6442 - accuracy: 0.6100\n",
      "Epoch 154/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6050\n",
      "Epoch 155/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6050\n",
      "Epoch 156/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6000\n",
      "Epoch 157/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6000\n",
      "Epoch 158/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6000\n",
      "Epoch 159/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6000\n",
      "Epoch 160/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.6000\n",
      "Epoch 161/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6432 - accuracy: 0.6050\n",
      "Epoch 162/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6431 - accuracy: 0.6100\n",
      "Epoch 163/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6050\n",
      "Epoch 164/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6430 - accuracy: 0.6050\n",
      "Epoch 165/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6100\n",
      "Epoch 166/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.6050\n",
      "Epoch 167/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6427 - accuracy: 0.6050\n",
      "Epoch 168/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6050\n",
      "Epoch 169/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6100\n",
      "Epoch 170/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.6100\n",
      "Epoch 171/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.6100\n",
      "Epoch 172/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6050\n",
      "Epoch 173/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6000\n",
      "Epoch 174/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6418 - accuracy: 0.6000\n",
      "Epoch 175/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6417 - accuracy: 0.6000\n",
      "Epoch 176/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.6000\n",
      "Epoch 177/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6416 - accuracy: 0.6000\n",
      "Epoch 178/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6050\n",
      "Epoch 179/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.6100\n",
      "Epoch 180/700\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.61 - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6100\n",
      "Epoch 181/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.6100\n",
      "Epoch 182/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6411 - accuracy: 0.6050\n",
      "Epoch 183/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6410 - accuracy: 0.6050\n",
      "Epoch 184/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6409 - accuracy: 0.6050\n",
      "Epoch 185/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6409 - accuracy: 0.6050\n",
      "Epoch 186/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.6050\n",
      "Epoch 187/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.6050\n",
      "Epoch 188/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.6050\n",
      "Epoch 189/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.6050\n",
      "Epoch 190/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6407 - accuracy: 0.6100\n",
      "Epoch 191/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.6100\n",
      "Epoch 192/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.6050\n",
      "Epoch 193/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6403 - accuracy: 0.6050\n",
      "Epoch 194/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6402 - accuracy: 0.6050\n",
      "Epoch 195/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6050\n",
      "Epoch 196/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6100\n",
      "Epoch 197/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6100\n",
      "Epoch 198/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6400 - accuracy: 0.6050\n",
      "Epoch 199/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6000\n",
      "Epoch 200/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6050\n",
      "Epoch 201/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.6050\n",
      "Epoch 202/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.6050\n",
      "Epoch 203/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6398 - accuracy: 0.6000\n",
      "Epoch 204/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.6000\n",
      "Epoch 205/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6396 - accuracy: 0.6050\n",
      "Epoch 206/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6396 - accuracy: 0.6000\n",
      "Epoch 207/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6395 - accuracy: 0.6000\n",
      "Epoch 208/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6394 - accuracy: 0.6000\n",
      "Epoch 209/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6393 - accuracy: 0.6050\n",
      "Epoch 210/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6393 - accuracy: 0.6050\n",
      "Epoch 211/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6392 - accuracy: 0.6050\n",
      "Epoch 212/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6392 - accuracy: 0.6050\n",
      "Epoch 213/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6050\n",
      "Epoch 214/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6050\n",
      "Epoch 215/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.6100\n",
      "Epoch 216/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.6100\n",
      "Epoch 217/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6100\n",
      "Epoch 218/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6050\n",
      "Epoch 219/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.6050\n",
      "Epoch 220/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6389 - accuracy: 0.6100\n",
      "Epoch 221/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6388 - accuracy: 0.6050\n",
      "Epoch 222/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6388 - accuracy: 0.6050\n",
      "Epoch 223/700\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6386 - accuracy: 0.6000\n",
      "Epoch 224/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6388 - accuracy: 0.6000\n",
      "Epoch 225/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.6050\n",
      "Epoch 226/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.6050\n",
      "Epoch 227/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.6000\n",
      "Epoch 228/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.6050\n",
      "Epoch 229/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 230/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 231/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 232/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 233/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6384 - accuracy: 0.6050\n",
      "Epoch 234/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6050\n",
      "Epoch 235/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 236/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6050\n",
      "Epoch 237/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 0.6050\n",
      "Epoch 238/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6050\n",
      "Epoch 239/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6050\n",
      "Epoch 240/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6050\n",
      "Epoch 241/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6382 - accuracy: 0.6000\n",
      "Epoch 242/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6050\n",
      "Epoch 243/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6386 - accuracy: 0.6000\n",
      "Epoch 244/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6050\n",
      "Epoch 245/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6382 - accuracy: 0.6050\n",
      "Epoch 246/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 247/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6050\n",
      "Epoch 248/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6050\n",
      "Epoch 249/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6100\n",
      "Epoch 250/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6050\n",
      "Epoch 251/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6050\n",
      "Epoch 252/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6050\n",
      "Epoch 253/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6050\n",
      "Epoch 254/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6050\n",
      "Epoch 255/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6050\n",
      "Epoch 256/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6050\n",
      "Epoch 257/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6379 - accuracy: 0.6050\n",
      "Epoch 258/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6050\n",
      "Epoch 259/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6050\n",
      "Epoch 260/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6050\n",
      "Epoch 261/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6000\n",
      "Epoch 262/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6379 - accuracy: 0.6100\n",
      "Epoch 263/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6050\n",
      "Epoch 264/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6050\n",
      "Epoch 265/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 266/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 267/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 268/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6050\n",
      "Epoch 269/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 270/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 271/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 272/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 273/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 274/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 275/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 276/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 277/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 278/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6378 - accuracy: 0.6050\n",
      "Epoch 279/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 280/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 281/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 282/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 283/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6000\n",
      "Epoch 284/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 285/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 286/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 287/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 288/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 289/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 290/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 291/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 292/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 293/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 294/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 295/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 296/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 297/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 298/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 299/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 300/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.5950\n",
      "Epoch 301/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 302/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 303/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 304/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 305/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6000\n",
      "Epoch 306/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 307/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.6000\n",
      "Epoch 308/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6000\n",
      "Epoch 309/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 310/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 311/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 312/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 313/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 314/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 315/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 316/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 317/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 318/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 319/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 320/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6100\n",
      "Epoch 321/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6100\n",
      "Epoch 322/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 323/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 324/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 325/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 326/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 327/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6000\n",
      "Epoch 328/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6000\n",
      "Epoch 329/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 330/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 331/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 332/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 333/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 334/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 335/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 336/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 337/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 338/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 339/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6000\n",
      "Epoch 340/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 341/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 342/700\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.60 - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 343/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 344/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 345/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 346/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 347/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 348/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 349/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 350/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 351/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6000\n",
      "Epoch 352/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 353/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 354/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 355/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 356/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 357/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 358/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 359/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 360/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 361/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 362/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 363/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 364/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 365/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 366/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 367/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 368/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 369/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 370/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 371/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 372/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 373/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 374/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 375/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.5950\n",
      "Epoch 376/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 377/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 378/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 379/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 380/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 381/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 382/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6200\n",
      "Epoch 383/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6200\n",
      "Epoch 384/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6200\n",
      "Epoch 385/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6200\n",
      "Epoch 386/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6150\n",
      "Epoch 387/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 388/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 389/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 390/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 391/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 392/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 393/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 394/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 395/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 396/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 397/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 398/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 399/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 400/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 401/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 402/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 403/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 404/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 405/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 406/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 407/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 408/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 409/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 410/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 411/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 412/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 413/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 414/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 415/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 416/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 417/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 418/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 419/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 420/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 421/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 422/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 423/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 424/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 425/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 426/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 427/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 428/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 429/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 430/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 431/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 432/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 433/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 434/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 435/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 436/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 437/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 438/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 439/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 440/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 441/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 442/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 443/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 444/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 445/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 446/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 447/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6000\n",
      "Epoch 448/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 449/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 450/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 451/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 452/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 453/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 454/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 455/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6100\n",
      "Epoch 456/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 457/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 458/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 459/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 460/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 461/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 462/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 463/700\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 464/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 465/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6000\n",
      "Epoch 466/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 467/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 468/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 469/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 470/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 471/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 472/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 473/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 474/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 475/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 476/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 477/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 478/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 479/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 480/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 481/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 482/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 483/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 484/700\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 485/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 486/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 487/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 488/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 489/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 490/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 491/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 492/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 493/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 494/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 495/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 496/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 497/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 498/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 499/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 500/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 501/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 502/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 503/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 504/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 505/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 506/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 507/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6371 - accuracy: 0.6050\n",
      "Epoch 508/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6377 - accuracy: 0.6050\n",
      "Epoch 509/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 510/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 511/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 512/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 513/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 514/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 515/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 516/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 517/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 518/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 519/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 520/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 521/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 522/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 523/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 524/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6000\n",
      "Epoch 525/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 526/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 527/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 528/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 529/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 530/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 531/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 532/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 533/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 534/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 535/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 536/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 537/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 538/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 539/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 540/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 541/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 542/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6000\n",
      "Epoch 543/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 544/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 545/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 546/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 547/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 548/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 549/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 550/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 551/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 552/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 553/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 554/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 555/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 556/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 557/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 558/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 559/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 560/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 561/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 562/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 563/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 564/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 565/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 566/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 567/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 568/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 569/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 570/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 571/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 572/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 573/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 574/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 575/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 576/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 577/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 578/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 579/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 580/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 581/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 582/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 583/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 584/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 585/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 586/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 587/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 588/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 589/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 590/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 591/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 592/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 593/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 594/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 595/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 596/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 597/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 598/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 599/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 600/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 601/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 602/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 603/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 604/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 605/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 606/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 607/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 608/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 609/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 610/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 611/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 612/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 613/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 614/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 615/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 616/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 617/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 618/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 619/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 620/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 621/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 622/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 623/700\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 624/700\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 625/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 626/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 627/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6000\n",
      "Epoch 628/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 629/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 630/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 631/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 632/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 633/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 634/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 635/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 636/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 637/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 638/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6200\n",
      "Epoch 639/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6200\n",
      "Epoch 640/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6200\n",
      "Epoch 641/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6200\n",
      "Epoch 642/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6200\n",
      "Epoch 643/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6200\n",
      "Epoch 644/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.6100\n",
      "Epoch 645/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 646/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 647/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 648/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 649/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 650/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 651/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 652/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 653/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 654/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 655/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 656/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 657/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 658/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 659/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 660/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 661/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 662/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 663/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 664/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 665/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 666/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 667/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 668/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 669/700\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 670/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 671/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 672/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 673/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 674/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 675/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 676/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 677/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 678/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 679/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 680/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6100\n",
      "Epoch 681/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6100\n",
      "Epoch 682/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 683/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 684/700\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 685/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 686/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 687/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 688/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 689/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 690/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 691/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 692/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6050\n",
      "Epoch 693/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 694/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6050\n",
      "Epoch 695/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6050\n",
      "Epoch 696/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n",
      "Epoch 697/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 698/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6000\n",
      "Epoch 699/700\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6050\n",
      "Epoch 700/700\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6050\n"
     ]
    }
   ],
   "source": [
    "# Paso 6: Entrenamiento del modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=700, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.71        23\n",
      "           1       0.83      0.54      0.65        28\n",
      "\n",
      "    accuracy                           0.69        51\n",
      "   macro avg       0.72      0.70      0.68        51\n",
      "weighted avg       0.73      0.69      0.68        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 7: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)  # Umbral de 0.5 para convertir a etiquetas binarias\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizar la pérdida y la precisión durante el entrenamiento\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "plt.legend()\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "\n",
    "# Precisión\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n",
    "plt.legend()\n",
    "plt.title('Precisión durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.69\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    140\n",
       "1    111\n",
       "Name: Tendence, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['Tendence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función que construye el modelo\n",
    "def create_model(neurons=16, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=1, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión: 0.6650 usando {'activation': 'tanh', 'batch_size': 128, 'epochs': 1000, 'neurons': 64}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Crear un modelo basado en KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Definir los hiperparámetros a optimizar\n",
    "param_grid = {\n",
    "    'neurons': [16, 32, 64],  # Prueba con diferentes números de neuronas\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],  # Prueba diferentes funciones de activación\n",
    "    'batch_size': [32, 64, 128],  # Prueba diferentes tamaños de lote\n",
    "    'epochs': [1000]  # Prueba diferentes números de épocas\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros\n",
    "print(f\"Mejor precisión: {grid_result.best_score_:.4f} usando {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"model.h5\")# Define el diseño de la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from textblob import TextBlob  # Asegúrate de tener instalada la biblioteca TextBlob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from PIL import Image, ImageTk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pabloneira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descarga las stopwords si aún no lo has hecho\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Paso 1: Limpieza y preprocesamiento de texto\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()  # Tokenización\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Eliminar stopwords\n",
    "\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    analysis = TextBlob(cleaned_text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def predecir_sentimiento():\n",
    "    tweet = tweet_entry.get()\n",
    "    sentiment = get_sentiment(tweet)\n",
    "    prediction = model.predict([sentiment])\n",
    "    resultado = \"Sube\" if prediction >= 0.5 else \"Baja\"  # Ajusta esto según tu modelo\n",
    "\n",
    "    resultado_label.config(text=f\"Predicción: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la ventana principal\n",
    "ventana = tk.Tk()\n",
    "ventana.title(\"NASDAQ PREDICTION BY TWEETS\")\n",
    "\n",
    "\n",
    "# Etiqueta y entrada para el tweet\n",
    "tweet_label = tk.Label(ventana, text=\"Ingrese un tweet:\")\n",
    "tweet_label.pack()\n",
    "tweet_entry = tk.Entry(ventana)\n",
    "tweet_entry.pack()\n",
    "\n",
    "# Botón para realizar la predicción\n",
    "prediccion_button = tk.Button(ventana, text=\"Obtener Predicción\", command=predecir_sentimiento)\n",
    "prediccion_button.pack()\n",
    "\n",
    "# Etiqueta para mostrar la predicción\n",
    "resultado_label = tk.Label(ventana, text=\"\")\n",
    "resultado_label.pack()\n",
    "\n",
    "ventana.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    # Paso 1: Limpieza y preprocesamiento de texto\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)  # Eliminar caracteres especiales y números\n",
    "    tokens = text.split()  # Tokenización\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Eliminar stopwords\n",
    "\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    analysis = TextBlob(cleaned_text)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pabloneira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Descarga las stopwords si aún no lo has hecho\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Paso 1: Limpieza y preprocesamiento de texto\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)  # Eliminar caracteres especiales y números\n",
    "    tokens = text.split()  # Tokenización\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Eliminar stopwords\n",
    "\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    analysis = TextBlob(cleaned_text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def set_background():\n",
    "    global background_photo\n",
    "    background_label = tk.Label(ventana, image=background_photo)\n",
    "    background_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "def set_logo():\n",
    "    global logo_photo\n",
    "    logo_label = tk.Label(ventana, image=logo_photo)\n",
    "    logo_label.place(x=10, y=10)\n",
    "\n",
    "def predecir_sentimiento():\n",
    "    tweet = tweet_entry.get(\"1.0\", \"end-1c\")  # Obtener todo el contenido del widget Text\n",
    "    sentiment = get_sentiment(tweet)\n",
    "    prediction = model.predict([sentiment])\n",
    "    resultado = \"Sube\" if prediction >= 0.5 else \"Baja\"  # Ajusta esto según tu modelo\n",
    "\n",
    "    resultado_label.config(text=f\"Predicción: {resultado}\", font=(\"Helvetica\", 16, \"bold\"))\n",
    "\n",
    "# Crear la ventana principal\n",
    "ventana = tk.Tk()\n",
    "ventana.title(\"Predicción de Sentimiento en Twitter\")\n",
    "\n",
    "# Personalizar el fondo de la ventana\n",
    "ventana.configure(bg=\"#1DA1F2\")  # Color de fondo similar al color de Twitter\n",
    "\n",
    "# Ajustar el tamaño de la ventana\n",
    "ventana.geometry(\"600x400\")  # Aumentar el tamaño de la ventana\n",
    "\n",
    "# Etiqueta y entrada para el tweet\n",
    "tweet_label = tk.Label(ventana, text=\"Ingresa un tweet:\", fg=\"white\", bg=\"#1DA1F2\", font=(\"Helvetica\", 16, \"bold\"))  # Aumentar el tamaño de fuente\n",
    "tweet_label.pack(pady=10)\n",
    "\n",
    "tweet_entry = tk.Text(ventana, font=(\"Helvetica\", 14), height=4, wrap=tk.WORD)  # Utilizar el widget Text para entrada de varias líneas\n",
    "tweet_entry.pack(padx=10, pady=10)\n",
    "\n",
    "# Botón para realizar la predicción\n",
    "prediccion_button = tk.Button(ventana, text=\"Obtener Predicción\", command=predecir_sentimiento, bg=\"#1DA1F2\", fg=\"black\", font=(\"Helvetica\", 14, \"bold\"))  # Aumentar el tamaño de fuente\n",
    "prediccion_button.pack(pady=10)\n",
    "\n",
    "# Etiqueta para mostrar la predicción\n",
    "resultado_label = tk.Label(ventana, text=\"\", fg=\"white\", bg=\"#1DA1F2\", font=(\"Helvetica\", 16, \"bold\"))\n",
    "resultado_label.pack(pady=10)\n",
    "\n",
    "ventana.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
